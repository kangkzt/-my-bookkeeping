/**
 * æ•°æ®å¯¼å…¥å¯¼å‡ºåŠŸèƒ½ (IndexedDB ç‰ˆæœ¬)
 */

import { getDB } from './database'
import { createClient } from 'webdav'
import Papa from 'papaparse'
import JSZip from 'jszip'
import { addTransaction, addTransactionsBatch } from './stores'

/**
 * å¯¼å‡ºæ‰€æœ‰æ•°æ®ä¸ºJSON
 * @param {boolean} includePhotos - æ˜¯å¦åŒ…å«å›¾ç‰‡
 */
export async function getDataCounts() {
    const db = getDB()
    const [t, c, tag, p, a, ph] = await Promise.all([
        db.count('transactions'),
        db.count('categories'),
        db.count('tags'),
        db.count('persons'),
        db.count('accounts'),
        db.count('photos')
    ])
    return {
        transactions: t,
        categories: c,
        tags: tag,
        persons: p,
        accounts: a,
        photos: ph
    }
}

export async function exportAllData(includePhotos = true) {
    const db = getDB()

    const data = {
        version: '1.0',
        exportDate: new Date().toISOString(),
        transactions: await db.getAll('transactions'),
        categories: await db.getAll('categories'),
        tags: await db.getAll('tags'),
        persons: await db.getAll('persons'),
        accounts: await db.getAll('accounts'),
        photos: includePhotos ? await db.getAll('photos') : []
    }

    return data
}

/**
 * å¯¼å‡ºä¸ºJSONæ–‡ä»¶å¹¶ä¸‹è½½
 */
export async function downloadExportFile(onProgress = () => { }) {
    onProgress(10)
    const data = await exportAllData()
    onProgress(50)
    const jsonStr = JSON.stringify(data, null, 2)
    onProgress(80)
    const blob = new Blob([jsonStr], { type: 'application/json' })

    const url = URL.createObjectURL(blob)
    const a = document.createElement('a')
    a.href = url
    a.download = `quickbook_backup_${new Date().toISOString().split('T')[0]}.json`
    document.body.appendChild(a)
    a.click()
    document.body.removeChild(a)
    URL.revokeObjectURL(url)
    onProgress(100)
}

/**
 * å¯¼å‡ºä¸ºCSVæ–‡ä»¶å¹¶ä¸‹è½½
 */
export async function downloadExportCSV(onProgress = () => { }) {
    onProgress(10)
    const db = getDB()
    const [transactions, categories, accounts, persons] = await Promise.all([
        db.getAll('transactions'),
        db.getAll('categories'),
        db.getAll('accounts'),
        db.getAll('persons')
    ])
    onProgress(40)

    const catMap = new Map(categories.map(c => [c.id, c.name]))
    const accMap = new Map(accounts.map(a => [a.id, a.name]))
    const personMap = new Map(persons.map(p => [p.id, p.name]))

    const csvData = transactions.map(t => ({
        'æ—¥æœŸ': t.date ? t.date.replace('T', ' ').slice(0, 16) : '',
        'ç±»å‹': t.type === 'expense' ? 'æ”¯å‡º' : t.type === 'income' ? 'æ”¶å…¥' : t.type === 'transfer' ? 'è½¬è´¦' : t.type,
        'é‡‘é¢': Number(t.amount).toFixed(2),
        'åˆ†ç±»': catMap.get(t.categoryId) || '',
        'å­ç±»åˆ«': t.subCategory || '',
        'è´¦æˆ·': accMap.get(t.accountId) || '',
        'è½¬å…¥è´¦æˆ·': t.toAccountId ? accMap.get(t.toAccountId) || '' : '',
        'æˆå‘˜': personMap.get(t.personId) || '',
        'å•†å®¶': t.merchant || '',
        'é¡¹ç›®': t.project || '',
        'å¤‡æ³¨': t.remark || ''
    }))
    onProgress(70)

    const csv = Papa.unparse(csvData)
    const blob = new Blob(["\uFEFF" + csv], { type: 'text/csv;charset=utf-8;' })

    const url = URL.createObjectURL(blob)
    const a = document.createElement('a')
    a.href = url
    a.download = `quickbook_export_${new Date().toISOString().split('T')[0]}.csv`
    document.body.appendChild(a)
    a.click()
    document.body.removeChild(a)
    URL.revokeObjectURL(url)
    onProgress(100)
}

/**
 * å¯¼å…¥JSONæ•°æ®
 */
export async function importData(jsonData, onProgress = () => { }) {
    const db = getDB()
    onProgress(10)

    // æ¸…ç©ºç°æœ‰æ•°æ®
    const tx = db.transaction(['transactions', 'categories', 'tags', 'persons', 'photos', 'accounts'], 'readwrite')

    await Promise.all([
        tx.objectStore('transactions').clear(),
        tx.objectStore('categories').clear(),
        tx.objectStore('tags').clear(),
        tx.objectStore('persons').clear(),
        tx.objectStore('photos').clear(),
        tx.objectStore('accounts').clear()
    ])
    onProgress(30)

    const stores = ['transactions', 'categories', 'tags', 'persons', 'photos', 'accounts']
    let currentStore = 0

    for (const storeName of stores) {
        const items = jsonData[storeName] || []
        if (items.length > 0) {
            const store = tx.objectStore(storeName)
            // æ‰¹é‡å†™å…¥ï¼Œæ¯ 100 æ¡ä¸€ç»„
            let batch = []
            for (let i = 0; i < items.length; i++) {
                // å¼ºåˆ¶é‡ç½®åŒæ­¥çŠ¶æ€ä¸º 0ï¼Œç¡®ä¿å¯¼å…¥çš„æ•°æ®ä¼šè¢«åŒæ­¥åˆ°äº‘ç«¯
                const item = { ...items[i], synced: 0 }
                batch.push(store.add(item))
                if (batch.length >= 100) {
                    await Promise.all(batch)
                    batch = []
                }
            }
            if (batch.length > 0) await Promise.all(batch)
        }

        currentStore++
        onProgress(30 + Math.round((currentStore / stores.length) * 70))
    }

    return true
}

/**
 * ä»æ–‡ä»¶å¯¼å…¥
 */
export function importFromFile() {
    return new Promise((resolve, reject) => {
        const input = document.createElement('input')
        input.type = 'file'
        input.accept = '.json'

        input.onchange = async (e) => {
            const file = e.target.files[0]
            if (!file) {
                reject(new Error('æœªé€‰æ‹©æ–‡ä»¶'))
                return
            }

            try {
                const text = await file.text()
                const data = JSON.parse(text)
                await importData(data)
                resolve(true)
            } catch (error) {
                reject(error)
            }
        }

        input.click()
    })
}

/**
 * å¯¼å…¥CSVæ•°æ® (æ”¯æŒéšæ‰‹è®°æ ¼å¼)
 */
export async function importCSVData(rows, onProgress = () => { }) {
    const db = getDB()
    let count = 0

    // è·å–åŸºç¡€æ•°æ®ç”¨äºåŒ¹é…
    let allCats = await db.getAll('categories')
    let allAccs = await db.getAll('accounts')
    let allMerchants = await db.getAll('merchants')
    let allPersons = await db.getAll('persons')

    // å»ºç«‹åç§°æ˜ å°„ (Name -> ID)
    const catMap = new Map()
    allCats.forEach(c => catMap.set(c.name, c.id))

    const accMap = new Map()
    allAccs.forEach(a => accMap.set(a.name, a.id))

    const merchantMap = new Map()
    allMerchants.forEach(m => merchantMap.set(m.name, m.id))

    const personMap = new Map()
    allPersons.forEach(p => personMap.set(p.name, p.id))

    // é»˜è®¤è´¦æˆ·å’Œåˆ†ç±» (å¦‚æœæ²¡æœ‰åŒ¹é…åˆ°)
    let defaultAccId = allAccs.length > 0 ? allAccs[0].id : null
    let defaultCatId = allCats.length > 0 ? allCats[0].id : null

    // å¦‚æœè¿˜æ²¡æœ‰å¿…è¦çš„æ•°æ®ï¼Œå…ˆåˆ›å»ºé»˜è®¤çš„
    if (!defaultAccId) {
        defaultAccId = await db.add('accounts', { name: 'ç°é‡‘', type: 'cash', balance: 0 })
        accMap.set('ç°é‡‘', defaultAccId)
    }
    if (!defaultCatId) {
        defaultCatId = await db.add('categories', { name: 'å…¶ä»–', type: 'expense', icon: 'ğŸ·ï¸' })
        catMap.set('å…¶ä»–', defaultCatId)
    }

    const tx = db.transaction(['merchants', 'persons', 'accounts'], 'readwrite')
    const merchantStore = tx.objectStore('merchants')
    const personStore = tx.objectStore('persons')
    const accountStore = tx.objectStore('accounts')

    // è¾…åŠ©å‡½æ•°ï¼šè·å–æˆ–åˆ›å»ºå•†å®¶
    const getOrCreateMerchant = async (name) => {
        if (!name) return ''
        if (merchantMap.has(name)) return name // Map save ID or Name? Here it seems name is used as ID or simple string
        // Wait, original code: merchantMap.set(name, id). But wait, merchant field in transaction stores String Name usually? 
        // Let's check original code. Original: if (merchantMap.has(name)) return name (Wait, if map has it, it returns name?)
        // Ah, line 215 originally: if (merchantMap.has(name)) return name. 
        // And line 219: merchantMap.set(name, id). 
        // This is inconsistent. If I return name, I am storing name. If I return ID, I store ID.
        // Let's look at schema. Transactions usually store merchant name as string, or ID?
        // In this app, it seems 'merchant' field is likely a string (name).
        // Let's assume it is string.
        return name
    }

    // è¾…åŠ©å‡½æ•°ï¼šè·å–æˆ–åˆ›å»ºæˆå‘˜
    const getOrCreatePerson = async (name) => {
        if (!name) return null
        if (personMap.has(name)) return personMap.get(name)

        const newP = { name, avatar: 'ğŸ‘¤' }
        const id = await personStore.add(newP)
        personMap.set(name, id) // Store ID
        return id
    }

    // è¾…åŠ©å‡½æ•°ï¼šè·å–æˆ–åˆ›å»ºè´¦æˆ·
    const getOrCreateAccount = async (name) => {
        if (!name) return defaultAccId
        if (accMap.has(name)) return accMap.get(name)

        const newA = { name, type: 'asset', balance: 0, icon: 'ğŸ’³' }
        const id = await accountStore.add(newA)
        accMap.set(name, id)
        return id
    }

    // 4. è·å–ç°æœ‰æ•°æ®è¿›è¡Œå»é‡æ ¡éªŒ
    const allExisting = await db.getAll('transactions')
    const signatureSet = new Set(allExisting.map(t => {
        // ç­¾åç”Ÿæˆè§„åˆ™ï¼šæ—¥æœŸ_é‡‘é¢_ç±»å‹_å¤‡æ³¨ (ç²—ç•¥å»é‡)
        // æ³¨æ„ï¼šè¿™é‡Œæ—¥æœŸä½¿ç”¨ ISO å­—ç¬¦ä¸²çš„å‰ 16 ä½ (YYYY-MM-DDTHH:mm) å¿½ç•¥ç§’å’Œæ¯«ç§’ï¼Œé˜²æ­¢ç»†å¾®å·®å¼‚
        return `${t.date.slice(0, 16)}_${t.amount}_${t.type}_${t.remark || ''}`
    }))

    const total = rows.length
    let batchData = []
    let skippedCount = 0

    for (let i = 0; i < total; i++) {
        const row = rows[i]

        try {
            // 1. æ—¥æœŸå¤„ç†
            let dateStr = row['äº¤æ˜“æ—¶é—´'] || row['æ—¥æœŸ'] || row['Date']
            if (!dateStr) continue
            // å°è¯•æ ‡å‡†åŒ–æ—¥æœŸ (æ”¯æŒ YYYY.MM.DD, YYYY/MM/DD)
            dateStr = dateStr.replace(/\./g, '-').replace(/\//g, '-')
            const date = new Date(dateStr)
            if (isNaN(date.getTime())) continue
            const isoDate = date.toISOString()

            // 2. ç±»å‹å¤„ç†
            const typeStr = row['äº¤æ˜“ç±»å‹'] || row['ç±»å‹'] || row['Type'] || 'æ”¯å‡º'
            let type = 'expense'
            if (typeStr.includes('æ”¶å…¥')) type = 'income'
            else if (typeStr.includes('è½¬è´¦')) type = 'transfer'
            else if (typeStr.includes('ä½™é¢')) type = 'balance'

            // 3. é‡‘é¢å¤„ç†
            const amountStr = row['é‡‘é¢'] || row['é‡‘é¢(å…ƒ)'] || row['Amount'] || '0'
            const amount = parseFloat(String(amountStr).replace(/[Â¥,]/g, ''))
            if (isNaN(amount)) continue

            // 4. åˆ†ç±»åŒ¹é… 
            const catName = row['åˆ†ç±»'] || row['ç±»åˆ«'] || row['äº¤æ˜“åˆ†ç±»'] || row['Category']
            const subCatName = row['å­åˆ†ç±»'] || row['å­ç±»åˆ«']

            // å°è¯•æŸ¥æ‰¾åˆ†ç±»
            let categoryId = defaultCatId
            if (subCatName && catMap.has(subCatName)) {
                categoryId = catMap.get(subCatName)
            } else if (catName && catMap.has(catName)) {
                categoryId = catMap.get(catName)
            }

            // 5. è´¦æˆ·åŒ¹é…
            const accName = row['è´¦æˆ·'] || row['è´¦æˆ·1'] || row['Account']
            let accountId = await getOrCreateAccount(accName)

            let toAccountId = null
            if (type === 'transfer') {
                const toAccName = row['è´¦æˆ·2'] || row['è½¬å…¥è´¦æˆ·'] || row['ç›®æ ‡è´¦æˆ·']
                if (toAccName) {
                    toAccountId = await getOrCreateAccount(toAccName)
                }
            }

            // 6. å•†å®¶/å¤‡æ³¨/æˆå‘˜/é¡¹ç›®
            const merchantNameRaw = row['å•†å®¶'] || row['äº¤æ˜“å¯¹è±¡'] || row['Merchant']
            const merchant = await getOrCreateMerchant(merchantNameRaw) // Usually just returns name

            const personNameRaw = row['æˆå‘˜'] || row['Member']
            const personId = await getOrCreatePerson(personNameRaw)

            const remark = row['å¤‡æ³¨'] || row['Remark'] || ''
            const project = row['é¡¹ç›®'] || ''

            // æ£€æŸ¥é‡å¤
            // æ„é€ å½“å‰äº¤æ˜“çš„ç­¾å
            const currentSig = `${isoDate.slice(0, 16)}_${Math.abs(amount)}_${type}_${remark}`

            if (signatureSet.has(currentSig)) {
                skippedCount++
                continue
            }

            // æ–°å¢ï¼šåŠ å…¥åˆ° Set ä¸­é˜²æ­¢åŒæ‰¹æ¬¡å†…é‡å¤
            signatureSet.add(currentSig)

            const transaction = {
                date: isoDate,
                type,
                amount: Math.abs(amount),
                categoryId,
                subCategory: subCatName || '',
                accountId,
                toAccountId,
                merchant,
                personId,
                remark,
                project,
                created_at: new Date().toISOString(),
                synced: 0
            }


            // Add to batch data
            batchData.push(transaction)
            count++

            // Process batch if full (Larger batch for efficiency)
            if (batchData.length >= 2000) {
                await addTransactionsBatch(batchData)
                batchData = []
                onProgress(Math.round((i / total) * 100))
            }

        } catch (e) {
            console.warn('Import Skip:', row, e)
        }
    }

    // Process remaining
    if (batchData.length > 0) {
        await addTransactionsBatch(batchData)
    }

    onProgress(100)
    return count
}

/**
 * WebDAV åŒæ­¥åŠŸèƒ½
 */

export async function checkWebDAVConnection(url, username, password) {
    try {
        const client = createClient(url, {
            username,
            password
        })
        await client.getDirectoryContents('/')
        return true
    } catch (error) {
        console.error('WebDAV Connection Failed:', error)
        throw new Error('è¿æ¥å¤±è´¥ï¼Œè¯·æ£€æŸ¥é…ç½®')
    }
}

export async function uploadToWebDAV(config, data, signal) {
    const { davUrl, davUser, davPassword } = config
    const fileName = `quickbook_backup_${new Date().toISOString().split('T')[0]}.json`
    const client = createClient(davUrl, { username: davUser, password: davPassword })

    const jsonStr = JSON.stringify(data, null, 2)

    // Check if aborted before starting upload
    if (signal && signal.aborted) {
        throw new Error('Aborted')
    }

    // Attempt to pass signal if library supports it in options. 
    // If specific library version doesn't support it, we can't easily cancel the network request, 
    // but we can start by checking signal.
    // Assuming 'webdav' library supports passing options with signal to putFileContents.
    await client.putFileContents(`/${fileName}`, jsonStr, { signal })
}

export async function downloadFromWebDAV(config) {
    const { davUrl, davUser, davPassword } = config
    const client = createClient(davUrl, { username: davUser, password: davPassword })

    // è·å–æœ€æ–°çš„å¤‡ä»½æ–‡ä»¶
    const items = await client.getDirectoryContents('/')
    const backupFiles = items
        .filter(i => i.filename.startsWith('quickbook_backup_') && i.filename.endsWith('.json'))
        .sort((a, b) => b.lastmod.localeCompare(a.lastmod)) // æŒ‰æ—¶é—´é™åº

    if (backupFiles.length === 0) {
        throw new Error('æœªæ‰¾åˆ°å¤‡ä»½æ–‡ä»¶')
    }

    const latestFile = backupFiles[0]
    const content = await client.getFileContents(latestFile.filename, { format: 'text' })
    return JSON.parse(content)
}

/**
 * å¯¼å‡ºå›¾ç‰‡ä¸º ZIP (ç‹¬ç«‹å¤‡ä»½ï¼ŒäºŒè¿›åˆ¶å­˜å‚¨æ›´çœç©ºé—´)
 */
export async function exportImagesAsZip(onProgress = () => { }) {
    onProgress(10)
    const db = getDB()
    const photos = await db.getAll('photos')

    if (photos.length === 0) throw new Error('æ²¡æœ‰å›¾ç‰‡å¯å¯¼å‡º')

    onProgress(30)
    const zip = new JSZip()
    const imgFolder = zip.folder("images")

    // Helper: Base64 to Blob
    const base64ToBlob = (dataURI) => {
        try {
            const splitDataURI = dataURI.split(',')
            const byteString = splitDataURI[0].indexOf('base64') >= 0 ? atob(splitDataURI[1]) : decodeURI(splitDataURI[1])
            const mimeString = splitDataURI[0].split(':')[1].split(';')[0]
            const ia = new Uint8Array(byteString.length)
            for (let i = 0; i < byteString.length; i++) {
                ia[i] = byteString.charCodeAt(i)
            }
            return new Blob([ia], { type: mimeString })
        } catch (e) {
            console.error('Blob conversion failed', e)
            return null
        }
    }

    let processed = 0
    for (const photo of photos) {
        if (!photo.data) continue
        const blob = base64ToBlob(photo.data)
        if (blob) {
            // Filename format: transactionId_photoId.jpg (or png)
            // Detect extension
            const ext = blob.type.split('/')[1] || 'jpg'
            imgFolder.file(`${photo.transactionId}_${photo.id}.${ext}`, blob)
        }
        processed++
        if (processed % 10 === 0) onProgress(30 + Math.round((processed / photos.length) * 40))
    }

    onProgress(80)
    const content = await zip.generateAsync({ type: "blob" }, (metadata) => {
        onProgress(80 + Math.round(metadata.percent * 0.2))
    })

    // Download
    const url = URL.createObjectURL(content)
    const a = document.createElement('a')
    a.href = url
    a.download = `quickbook_images_${new Date().toISOString().split('T')[0]}.zip`
    document.body.appendChild(a)
    a.click()
    document.body.removeChild(a)
    URL.revokeObjectURL(url)
    onProgress(100)
}

/**
 * ä» ZIP å¯¼å…¥å›¾ç‰‡
 */
export async function importImagesFromZip(file, onProgress = () => { }) {
    onProgress(10)
    const db = getDB()
    const zip = await JSZip.loadAsync(file)
    const imgFolder = zip.folder("images")

    if (!imgFolder) throw new Error('ZIP æ–‡ä»¶æ ¼å¼ä¸æ­£ç¡® (æœªæ‰¾åˆ° images æ–‡ä»¶å¤¹)')

    const files = []
    imgFolder.forEach((relativePath, zipEntry) => {
        if (!zipEntry.dir) files.push(zipEntry)
    })

    if (files.length === 0) return 0

    onProgress(20)
    const tx = db.transaction('photos', 'readwrite')

    // Clear existing photos? Maybe not forcibly clear, but merge?
    // User requested "Restore", usually implies overwrite or addition. 
    // Let's just add/overwrite if ID exists (ID is in filename).
    // Actually IDB auto-increment IDs might conflict if we use the ID from filename as key.
    // However, if we preserve IDs, we risk conflict if we didn't clear.
    // Safe approach: Clear photos if doing a full restore, or just add.
    // Let's assume this is a restore operation. We'll try to keep transactionId links valid.

    let count = 0
    for (const zipEntry of files) {
        // Filename: transactionId_photoId.ext
        const filename = zipEntry.name.split('/').pop() // remove folder prefix if any
        const [namePart] = filename.split('.')
        // namePart might be "transId_photoId"
        const parts = namePart.split('_')
        if (parts.length < 2) continue

        const transactionId = Number(parts[0]) // transId
        // We typically ignore the old photo ID and let IDB generate a new one, OR we try to reuse it?
        // If "transactionId" is valid, that's what matters.
        // But if the transactions were also imported, their IDs might be preserved.

        try {
            const blob = await zipEntry.async("blob")
            // Convert Blob back to Base64
            const reader = new FileReader()
            const base64Promise = new Promise((resolve) => {
                reader.onloadend = () => resolve(reader.result)
                reader.readAsDataURL(blob)
            })
            const base64Data = await base64Promise

            await tx.store.add({
                transactionId: transactionId,
                data: base64Data,
                synced: 0,
                updatedAt: Date.now()
            })
            count++
            if (count % 10 === 0) onProgress(20 + Math.round((count / files.length) * 70))
        } catch (e) {
            console.error('Import image failed', filename, e)
        }
    }
    await tx.done
    onProgress(100)
    return count
}
